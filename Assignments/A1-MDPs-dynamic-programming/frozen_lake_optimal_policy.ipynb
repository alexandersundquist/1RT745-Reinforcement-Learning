{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9ff7c4",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cea9d7",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bb2f9",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Consider the FrozenLake8x8-v1 environment. It is similar to the FrozenLake-v1 that was studied\n",
    "in Tinkering Notebook 2, but it consist of an 8 × 8 grid and thus have 64 states.\n",
    "\n",
    "Write a code that find an optimal policy π∗(s) and the corresponding value function v∗(s).\n",
    "\n",
    "In the quizz on you will be asked for example ”Which of these are optimal actions in state s = 26?”\n",
    "or ”What is v∗(26)?”. So make sure that you can easily run code that can answer these types of\n",
    "questions for different states.\n",
    "\n",
    "Hint: You can check that your code seems to be working by ensuring that you get to correct answer\n",
    "to the following:\n",
    "\n",
    "- For the optimal policy v∗(26) = 0.80 (rounded to two decimals).\n",
    "- In s = 26 the optimal action is 0 (left).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da216b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for this notebook\n",
    "import gymnasium as gym\n",
    "import gym_RLcourse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6409541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():\n",
    "    \n",
    "    def __init__(self, nA=4, nS=64):\n",
    "        self.nA = nA # Number of actions\n",
    "        self.nS = nS # Number of states\n",
    "        \n",
    "        # Uniform probabilities in each state.\n",
    "        # That is, in each of the nS states\n",
    "        # each of the nA actions has probability\n",
    "        # 1/nA.\n",
    "        self.probs = np.ones((nS,nA))/nA \n",
    "\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(self.nA, p=self.probs[state]) \n",
    "        return action # a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "76f4ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agent(env, agent):\n",
    "    state, info = env.reset()\n",
    "    time_step = 0\n",
    "    total_reward = 0\n",
    "    truncated = False\n",
    "    terminated = False\n",
    "    while not truncated and not terminated:\n",
    "        action = agent.act(state);\n",
    "        state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        time_step += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        print(\"Time step:\", time_step)\n",
    "        print(\"State:\", state)\n",
    "        print(\"Action:\", action)\n",
    "        print(\"Total reward:\", total_reward)\n",
    "        \n",
    "    if truncated:\n",
    "        print(\"The environment was truncated even though a terminal state was not reached.\")\n",
    "    elif terminated:\n",
    "        print(\"A terminal state was reached.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cefb9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02186f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(64)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', render_mode=\"human\")\n",
    "state, info = env.reset()\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8233344a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 97\n",
      "State: 41\n",
      "Action: 1\n",
      "Total reward: 0.0\n",
      "A terminal state was reached.\n"
     ]
    }
   ],
   "source": [
    "agent = RandomAgent()\n",
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f25db8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1759a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_value(env, discount, s, a, v):\n",
    "    \n",
    "    action_value = 0\n",
    "    \n",
    "    # Loop through all possible (s', r) pairs\n",
    "    for p, next_s, reward, _ in env.unwrapped.P[s][a]:\n",
    "        action_value += p * (reward + discount * v[next_s])\n",
    "    \n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c70ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_RHS(env, discount, agent, s, v):\n",
    "    \n",
    "    state_value = 0\n",
    "    \n",
    "    for a in range(env.action_space.n):\n",
    "        # Loop through all possible actions\n",
    "        state_value += agent.probs[s][a] * compute_action_value(env, discount, s, a, v)\n",
    "    \n",
    "    return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1b455eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bellman_RHS_all(env, discount, agent, v0):\n",
    "    # v0 is the given value function\n",
    "    # v will be the right-hand side of the Bellman equation\n",
    "    # If v0 is indeed the value function, then we should get v = v0.\n",
    "    \n",
    "    v = np.zeros(env.observation_space.n)\n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        v[s] = Bellman_RHS(env, discount, agent, s, v0)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "095dd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, discount, agent, v0, max_iter=1000, tol=1e-6):\n",
    "    \n",
    "    v_old = v0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        v_new = Bellman_RHS_all(env, discount, agent, v_old)\n",
    "        \n",
    "        if np.max(np.abs(v_new-v_old)) < tol:\n",
    "            break\n",
    "            \n",
    "        v_old = v_new\n",
    "        \n",
    "    return v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54700aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.88195e-03 2.14944e-03 2.79688e-03 4.10391e-03 6.53057e-03 9.78456e-03\n",
      "  1.34274e-02 1.59485e-02]\n",
      " [1.61811e-03 1.77291e-03 2.14039e-03 2.98713e-03 5.70622e-03 9.39901e-03\n",
      "  1.45526e-02 1.84735e-02]\n",
      " [1.20269e-03 1.18669e-03 1.00713e-03 0.00000e+00 3.91045e-03 7.55536e-03\n",
      "  1.69135e-02 2.49228e-02]\n",
      " [8.05879e-04 7.66260e-04 7.02931e-04 7.70972e-04 2.38134e-03 0.00000e+00\n",
      "  2.06255e-02 3.93840e-02]\n",
      " [4.50526e-04 3.71089e-04 2.68386e-04 0.00000e+00 4.84438e-03 1.15929e-02\n",
      "  2.62057e-02 7.26052e-02]\n",
      " [1.75712e-04 0.00000e+00 0.00000e+00 1.44822e-03 5.40353e-03 1.53216e-02\n",
      "  0.00000e+00 1.52227e-01]\n",
      " [7.70750e-05 0.00000e+00 1.09332e-04 3.89386e-04 0.00000e+00 4.42900e-02\n",
      "  0.00000e+00 3.84076e-01]\n",
      " [5.57307e-05 3.45388e-05 4.79484e-05 0.00000e+00 5.39462e-02 1.61839e-01\n",
      "  3.87280e-01 0.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=5, suppress=False)\n",
    "env = gym.make('FrozenLake8x8-v1')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "# Write code for computing the state-value function\n",
    "v0 = np.zeros((env.observation_space.n))\n",
    "v = policy_evaluation(env, discount, agent, v0)\n",
    "print(v.reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14372f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(env, discount, agent, v):\n",
    "    \n",
    "    # The new policy will be a_probs\n",
    "    # We start by setting all probabilities to 0\n",
    "    # Then, when we have found the greedy action in a state, \n",
    "    # we change the probability for that action to 1.0.\n",
    "    \n",
    "    a_probs = np.zeros((env.observation_space.n, env.action_space.n)) \n",
    "    \n",
    "    for s in range(env.observation_space.n):\n",
    "        \n",
    "        action_values = np.zeros(env.action_space.n)\n",
    "        \n",
    "        for a in range(env.action_space.n):\n",
    "            # Compute action value for all actions\n",
    "            action_values[a] = compute_action_value(env, discount, s, a, v)\n",
    "            \n",
    "        a_max = np.argmax(action_values) # A greedy action\n",
    "        a_probs[s][a_max] = 1.0 # Always choose a greedy action!\n",
    "        \n",
    "    return a_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf9c77d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999 0.99999 1.      1.      1.      1.      1.      1.     ]\n",
      " [0.99999 0.99999 1.      1.      1.      1.      1.      1.     ]\n",
      " [0.99998 0.97819 0.92642 0.      0.85661 0.94623 0.98208 1.     ]\n",
      " [0.99997 0.93458 0.80107 0.4749  0.62362 0.      0.94468 1.     ]\n",
      " [0.99996 0.82559 0.54222 0.      0.53934 0.61119 0.85195 1.     ]\n",
      " [0.99996 0.      0.      0.16804 0.38321 0.44227 0.      1.     ]\n",
      " [0.99995 0.      0.19466 0.1209  0.      0.3324  0.      1.     ]\n",
      " [0.99995 0.73152 0.46309 0.      0.27747 0.55493 0.77747 0.     ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v1', render_mode='human')\n",
    "agent = RandomAgent()\n",
    "discount = 1\n",
    "# Enter code here\n",
    "v_old = np.zeros(env.observation_space.n) \n",
    "for i in range(1000):\n",
    "    v = policy_evaluation(env, discount, agent, v_old)\n",
    "\n",
    "    if (np.max(np.abs(v-v_old))<1e-6):\n",
    "        break\n",
    "\n",
    "    v_old = v\n",
    "    agent.probs = greedy_policy(env, discount, agent, v)\n",
    "\n",
    "print(v.reshape(8,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b10fa377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time step: 92\n",
      "State: 63\n",
      "Action: 2\n",
      "Total reward: 1.0\n",
      "A terminal state was reached.\n"
     ]
    }
   ],
   "source": [
    "run_agent(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2314770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2597e121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
